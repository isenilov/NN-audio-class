{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks course project: #\n",
    "# Google AudioSet sound classification with Deep Learning #\n",
    "### Sapienza University of Rome ###\n",
    "\n",
    "### by Ivan Senilov (1787618) ###\n",
    "\n",
    "## 1. Introduction ##\n",
    "\n",
    "This work represents a practical part of the Neural Networks course taught at Sapienza University.\n",
    "\n",
    "The goal of this coursework is to:\n",
    "\n",
    "1. Explore the [Google AudioSet](https://research.google.com/audioset/index.html)\n",
    "2. Build the classification model based on Neural Network(s)\n",
    "3. Validate the model\n",
    "4. Evaluate and discuss the results\n",
    "\n",
    "## 2. Audio classification problem ##\n",
    "\n",
    "### 2.1 Audio features ###\n",
    "\n",
    "Feature extraction is signal processing task of computing the numerical representation from the signal that can be used to characterize the audio segment \\cite{prasad2007speech}.\n",
    "\n",
    "Most of the audio features fall into three categories \\cite{1221332}:\n",
    "\n",
    "1. **Energy-based**. For example, 4Hz modulation energy used for speech/music classification \\cite{scheirer1997construction}.\n",
    "2. **Spectrum-based**. Examples of the category are roll-off of the spectrum, spectral flux, Mel Frequency Cepstral Coefficents (MFCC) \\cite{scheirer1997construction} and linear spectrum pair, band periodicity \\cite{lu2001content}.\n",
    "3. **Perceptual based**, like pitch (estimated to discriminate songs and speech over music \\cite{zhang1998content}).\n",
    "\n",
    "The most developed areas of machine learning for audio classification include speech \\cite{shrawankar2013techniques,lee2003optimizing,ghahremani2014pitch} and music \\cite{logan2000mel,wang2006shazam,eronen2001comparison} recognition where  MFCCs are widely used as features. MFCCs were introduced in 1980 \\cite{davis1980comparison} and showed better performance in terms of recognition of spoken words. However, when it comes to other types of sound recognition, the selection of feature extraction method becomes less obvious, even though MFCCs are used in, for example, application to environmental sound classification \\cite{chu2009environmental,cotton2011spectral}.\n",
    "\n",
    "<img src=\"figs/feat_extr.png\">\n",
    "\n",
    "Extraction pipelines for MPEG-7 (left) and MFCC (right) features (reprinted from \\cite{1221332})\n",
    "\n",
    "Typical approach of feature extraction process is to split the audio signal into small chunks of several $ms$ (exact size is domain-dependent) and feed it into computational function of one of many frameworks for extraction of features from audio (see Figure \\ref{fig:feature_extraction} for examples of extraction pipelines for MPEG-7 \\cite{casey2001mpeg} and MFCC features). The most popular frameworks include YAAFE \\cite{mathieu2010yaafe} and openSMILE \\cite{eyben2010opensmile,eyben2013recent}, which allows to extract following feature types:\n",
    "\n",
    "1. Amplitude Modulation  \\cite{eronen2001automatic}. Analyzed frequency ranges are: Tremolo (4 - 8 Hz) and Grain (10 - 40 Hz). For each of these ranges, it computes:\n",
    "    * Frequency of maximum energy in range\n",
    "    * Difference of the energy of this frequency and the mean energy over all frequencies\n",
    "    * Difference of the energy of this frequency and the mean energy in range\n",
    "    * Product of the two first values.\n",
    "2. Autocorrelation coefficients $\\mathit{ac}$ on each frame.\n",
    "    $$ac(k)=\\sum_{i=0}^{N-k-1}x(i)x(i+k),$$    \n",
    "    where (here and below) $k$ is frame length in samples, $N$ is length of whole signal in samples and $x(i)$ is signal function.    \n",
    "3. Onset detection using a complex domain spectral flux method. \\cite{duxbury2003complex}.    \n",
    "4. Energy $\\mathit{en}$ as root mean square of an audio frame.\n",
    "    $$en=\\sqrt{\\dfrac{\\sum_{i=0}^{N-1}x(i)^2}{N}}$$    \n",
    "5. Envelope of an oscillating signal (smooth curve outlining its extremes).    \n",
    "6. Shape statistics (centroid, spread, skewness and kurtosis) of each frameâ€™s Temporal Shape, Amplitude Envelope and Magnitude Spectrum.    \n",
    "7. Linear Predictor Coefficients (LPC) of a signal frame \\cite{makhoul1975linear}.    \n",
    "8. Line Spectral Frequency (LSF) coefficients of a signal frame \\cite{backstrom2006properties,schussler1976stability}.    \n",
    "9. Loudness coefficients \\cite{moore1997model}.    \n",
    "10. Mel-frequencies cepstrum coefficients and Mel-frequencies spectrum.    \n",
    "11. Magnitude spectrum.    \n",
    "12. Octave band signal intensity (OBSI) using a triangular octave filter bank and  OBSI ratio between consecutive octave.\n",
    "13. Sharpness and Spread of Loudness coefficients \\cite{peeters2004large}.    \n",
    "14. Spectral crest factor per log-spaced band of 1/4 octave.    \n",
    "15. Spectral decrease, spectral flatness.    \n",
    "16. Spectral Flux.    \n",
    "17. Spectral roll-off (frequency so that 99\\% of the energy is contained below) \\cite{scheirer1997construction}.\n",
    "18. Spectral Slope (computed by linear regression of the spectral amplitude) \\cite{peeters2004large}.    \n",
    "19. Spectral Variation (normalized correlation of spectrum between consecutive frames) \\cite{peeters2004large}.    \n",
    "20. Zero-crossing rate (ZCR) for frame \\cite{scheirer1997construction}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation ##\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Load ontology and correspondence between video's ids and class labels.\n",
    "2. Download the videos from Youtube to respective directories\n",
    "3. Load labeled dataset into Numpy arrays\n",
    "4. Build classifier based on Neural Network architecture\n",
    "5. Evaluate it using cross-validation\n",
    "\n",
    "First of all, import data manipulation libs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify the task (in terms of computational complexity as well) we limit number of categories to 3. Read the ontology to Pandas dataframe and select only *id*, *name* and *child_id* fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_of_interest = (\"Vehicle\", \"Channel, environment and background\", \"Natural sounds\")\n",
    "\n",
    "with open(\"ontology.json\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "\n",
    "ontology = pd.read_json(contents)\n",
    "\n",
    "ontology = ontology[[\"id\", \"name\", \"child_ids\"]].set_index(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we extract *id*s for each (main and child) category (class) of interest recursively, building a dictionary where key is class name and value is a list of *id*s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ids(cls, data):  # recursively add all child classes of input class to the list\n",
    "    out = [] \n",
    "    for id, row in data.iterrows():\n",
    "        if row[\"name\"] == cls:\n",
    "            out.append(id)\n",
    "            if len(row[\"child_ids\"]) > 0:\n",
    "                for child in row[\"child_ids\"]:\n",
    "                    out.append(extract_ids(data[\"name\"][child], data))\n",
    "                \n",
    "    return flattern(out)\n",
    "\n",
    "\n",
    "def flattern(A):  # list flattening helper function\n",
    "    rt = []\n",
    "    for i in A:\n",
    "        if isinstance(i,list): rt.extend(flattern(i))\n",
    "        else: rt.append(i)\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {i:[] for i in classes_of_interest}\n",
    "for cls in classes_of_interest:\n",
    "    classes[cls] = extract_ids(cls, ontology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read tables with correspondance between YouTube ID, segment in the video and ids from ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_seconds</th>\n",
       "      <th>end_seconds</th>\n",
       "      <th>positive_labels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># YTID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>--PJHxphWEs</th>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[/m/09x0r, /t/dd00088]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--ZhevVpy1s</th>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>[/m/012xff]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--aE2O5G5WE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[/m/03fwl, /m/04rlf, /m/09x0r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--aO5cdqSAg</th>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[/t/dd00003, /t/dd00005]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--aaILOrkII</th>\n",
       "      <td>200.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>[/m/032s66, /m/073cg4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             start_seconds  end_seconds                 positive_labels\n",
       "# YTID                                                                 \n",
       "--PJHxphWEs           30.0         40.0          [/m/09x0r, /t/dd00088]\n",
       "--ZhevVpy1s           50.0         60.0                     [/m/012xff]\n",
       "--aE2O5G5WE            0.0         10.0  [/m/03fwl, /m/04rlf, /m/09x0r]\n",
       "--aO5cdqSAg           30.0         40.0        [/t/dd00003, /t/dd00005]\n",
       "--aaILOrkII          200.0        210.0          [/m/032s66, /m/073cg4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"balanced_train_segments.csv\", skiprows=2, sep=\", \", engine=\"python\", index_col=\"# YTID\")\n",
    "train_data[\"positive_labels\"] = train_data[\"positive_labels\"].apply(lambda x: x.replace('\\\"','').split(\",\"))\n",
    "\n",
    "test_data = pd.read_csv(\"eval_segments.csv\", skiprows=2, sep=\", \", engine=\"python\", index_col=\"# YTID\")\n",
    "test_data[\"positive_labels\"] = test_data[\"positive_labels\"].apply(lambda x: x.replace('\\\"','').split(\",\"))\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we download segments of videos from YouTube to respective folders of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\youtube_dl\\extractor\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlazy_extractors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlazy_extractors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_ALL_CLASSES\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'youtube_dl.extractor.lazy_extractors'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b884d6a2f68b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0myoutube_dl\u001b[0m   \u001b[1;31m# Python library for downloading from YouTube\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m '''\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\youtube_dl\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mFileDownloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m )\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mextractor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgen_extractors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_extractors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madobepass\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMSO_INFO\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mYoutubeDL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mYoutubeDL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\youtube_dl\\extractor\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0m_LAZY_LOADER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mextractors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     _ALL_CLASSES = [\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\youtube_dl\\extractor\\extractors.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1291\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mwistia\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWistiaIE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mworldstarhiphop\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWorldStarHipHopIE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1293\u001b[1;33m from .wrzuta import (\n\u001b[0m\u001b[0;32m   1294\u001b[0m     \u001b[0mWrzutaIE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \u001b[0mWrzutaPlaylistIE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import youtube_dl   # Python library for downloading from YouTube\n",
    "import os\n",
    "\n",
    "\n",
    "'''\n",
    "Helper function for cropping audio with ffmpeg\n",
    "'''\n",
    "def crop(start, length, filename):\n",
    "    command = \"ffmpeg -y -i \" + filename + \\\n",
    "    \" -ss  \" + str(start) + \" -t \" + str(length) + \\\n",
    "    \" -ac 1 -acodec copy \" + filename.split(\".\")[0] + \"_.wav\"\n",
    "    os.system(command)\n",
    "\n",
    "to_download = {i:[] for i in classes_of_interest}\n",
    "    \n",
    "for cls in classes_of_interest:\n",
    "    for id in classes[cls]:\n",
    "        for row in train_data.itertuples(): \n",
    "            if row[3][0] == id:\n",
    "                to_download[cls].append(row[0])\n",
    "                \n",
    "\n",
    "'''\n",
    "options for youtube-dl\n",
    "'''\n",
    "options = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'wav'   # wav format for lossless features extraction\n",
    "    }],\n",
    "    'extractaudio' : True,\n",
    "    'ignoreerrors' : True,\n",
    "    'audioformat' : \"wav\",\n",
    "    'noplaylist' : True,    # only download single clip, not playlist\n",
    "}\n",
    "                \n",
    "for cls in to_download:\n",
    "    # setting path for download removing commas and spaces in order to avoid fylesisitem access problems\n",
    "    options['outtmpl'] = os.path.join(cls.replace(\",\",\"\").replace(\" \",\"\"), '%(id)s.%(ext)s')\n",
    "    for file in to_download[cls]:\n",
    "        with youtube_dl.YoutubeDL(options) as ydl:\n",
    "            ydl.download([file])   # downloading full audio clip (due to limitations of youtube-dl)\n",
    "            filename = os.path.join(cls.replace(\",\",\"\").replace(\" \",\"\"), file + \".wav\")\n",
    "            crop(train_data.loc[file][\"start_seconds\"],\n",
    "                 10, filename)     # cropping of the clip in accordance with dataset csv file \n",
    "            try:\n",
    "                os.remove(filename)   # removing original (non-cropped) clip\n",
    "            except OSError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we extract features with help of SciPy and librosa (for MFCC extraction) libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import wavfile\n",
    "from librosa import feature\n",
    "\n",
    "# Removing commas and spaces from classes names for avoiding problems woth dir names \n",
    "classes_dirs = [i.replace(\",\",\"\").replace(\" \",\"\") for i in classes_of_interest]\n",
    "\n",
    "feat = []\n",
    "labels = []\n",
    "class_num = 0\n",
    "window = 32768                          # size of window for each sample\n",
    "for directory in classes_dirs:\n",
    "    count = 0\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.path.join(directory, file)\n",
    "        if os.path.isfile(filename) and count < 1000:    # limiting number of files to read\n",
    "            count += 1\n",
    "            rate, frames = wavfile.read(filename)\n",
    "            if len(frames.shape) > 1:    # if stereo, take only one channel\n",
    "                frames = frames[:,0]\n",
    "            for i in range(0, len(frames)-window, int(window/2)):\n",
    "                pxx = feature.mfcc(y=frames[i:i + window - 1], sr=rate, n_mfcc=20)\n",
    "                feat.append(pxx)\n",
    "                labels.append(class_num)\n",
    "    class_num += 1                          # each successive class is represented by incremented integer\n",
    "data = np.stack(feat)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(labels)             # convert class number to 'one-hot' vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 20, 64, 64)        1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 10, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 6, 28, 128)        204928    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 206,850\n",
      "Trainable params: 206,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "22125/22125 [==============================] - 87s 4ms/step - loss: 7.0774 - acc: 0.5599\n",
      "Epoch 2/100\n",
      "22125/22125 [==============================] - 85s 4ms/step - loss: 7.0774 - acc: 0.5609\n",
      "Epoch 3/100\n",
      "22125/22125 [==============================] - 85s 4ms/step - loss: 7.0774 - acc: 0.5609\n",
      "Epoch 4/100\n",
      "19200/22125 [=========================>....] - ETA: 11s - loss: 7.0643 - acc: 0.5617"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-882f7282ec42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                  verbose=1)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Conv2D, Dense, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "# from keras.callbacks import TensorBoard, ModelCheckpoint, Callback\n",
    "from keras import optimizers, regularizers\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size=5, strides=1, padding=\"same\", activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Conv2D(filters=128, kernel_size=5))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(X_train, y_train,\n",
    "                 # callbacks=[tbCallback, mcCallback, testCallback0, testCallback1, testCallback2, testCallback3],\n",
    "                 # validation_data=(X_1, y_1),\n",
    "                 epochs=100,\n",
    "                 batch_size=128,\n",
    "                 verbose=1)\n",
    "\n",
    "\n",
    "print(\"\\nEvaluating...\")\n",
    "y_pred = to_categorical(model.predict_classes(X_test, verbose=1))\n",
    "print(y_pred, y_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy:\", acc)\n",
    "rec = recall_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"Recall:\", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the experiments we found out that ReLU activation function in RNN layers results in model stop learning which is most propably consequence of vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_19 (Bidirectio (None, 20, 432)           485568    \n",
      "_________________________________________________________________\n",
      "bidirectional_20 (Bidirectio (None, 20, 432)           1121472   \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 432)               1121472   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 866       \n",
      "=================================================================\n",
      "Total params: 2,729,378\n",
      "Trainable params: 2,729,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 22125 samples, validate on 5532 samples\n",
      "Epoch 1/100\n",
      "22125/22125 [==============================] - 161s 7ms/step - loss: 0.6267 - acc: 0.6381 - val_loss: 0.6026 - val_acc: 0.6605\n",
      "Epoch 2/100\n",
      "22125/22125 [==============================] - 155s 7ms/step - loss: 0.5809 - acc: 0.6898 - val_loss: 0.5952 - val_acc: 0.6793\n",
      "Epoch 3/100\n",
      "22125/22125 [==============================] - 156s 7ms/step - loss: 0.5492 - acc: 0.7143 - val_loss: 0.6077 - val_acc: 0.6661\n",
      "Epoch 4/100\n",
      "22125/22125 [==============================] - 154s 7ms/step - loss: 0.4992 - acc: 0.7510 - val_loss: 0.6172 - val_acc: 0.6837\n",
      "Epoch 5/100\n",
      "22125/22125 [==============================] - 154s 7ms/step - loss: 0.4392 - acc: 0.7892 - val_loss: 0.6738 - val_acc: 0.6734\n",
      "Epoch 6/100\n",
      "22125/22125 [==============================] - 152s 7ms/step - loss: 0.3708 - acc: 0.8264 - val_loss: 0.7120 - val_acc: 0.6916\n",
      "Epoch 7/100\n",
      "22125/22125 [==============================] - 152s 7ms/step - loss: 0.3048 - acc: 0.8640 - val_loss: 0.8166 - val_acc: 0.6804\n",
      "Epoch 8/100\n",
      "22125/22125 [==============================] - 153s 7ms/step - loss: 0.2428 - acc: 0.8960 - val_loss: 0.8843 - val_acc: 0.6687\n",
      "Epoch 9/100\n",
      "22125/22125 [==============================] - 155s 7ms/step - loss: 0.1915 - acc: 0.9205 - val_loss: 0.9280 - val_acc: 0.6913\n",
      "Epoch 10/100\n",
      "22125/22125 [==============================] - 156s 7ms/step - loss: 0.1508 - acc: 0.9396 - val_loss: 1.0411 - val_acc: 0.7088\n",
      "Epoch 11/100\n",
      "22125/22125 [==============================] - 150s 7ms/step - loss: 0.1183 - acc: 0.9535 - val_loss: 1.0607 - val_acc: 0.6952\n",
      "Epoch 12/100\n",
      "22125/22125 [==============================] - 150s 7ms/step - loss: 0.1003 - acc: 0.9620 - val_loss: 1.1074 - val_acc: 0.7016\n",
      "Epoch 13/100\n",
      "22125/22125 [==============================] - 152s 7ms/step - loss: 0.0834 - acc: 0.9679 - val_loss: 1.2706 - val_acc: 0.6972\n",
      "Epoch 14/100\n",
      "22125/22125 [==============================] - 152s 7ms/step - loss: 0.0798 - acc: 0.9691 - val_loss: 1.2218 - val_acc: 0.7010\n",
      "Epoch 15/100\n",
      "22125/22125 [==============================] - 152s 7ms/step - loss: 0.0709 - acc: 0.9738 - val_loss: 1.1904 - val_acc: 0.7202\n",
      "Epoch 16/100\n",
      "22125/22125 [==============================] - 153s 7ms/step - loss: 0.0526 - acc: 0.9800 - val_loss: 1.2772 - val_acc: 0.7176\n",
      "Epoch 17/100\n",
      "22125/22125 [==============================] - 152s 7ms/step - loss: 0.0445 - acc: 0.9837 - val_loss: 1.2707 - val_acc: 0.7082\n",
      "Epoch 18/100\n",
      "22125/22125 [==============================] - 151s 7ms/step - loss: 0.0534 - acc: 0.9802 - val_loss: 1.3833 - val_acc: 0.7137\n",
      "Epoch 19/100\n",
      "22125/22125 [==============================] - 150s 7ms/step - loss: 0.0446 - acc: 0.9841 - val_loss: 1.2832 - val_acc: 0.7213\n",
      "Epoch 20/100\n",
      "22125/22125 [==============================] - 152s 7ms/step - loss: 0.0409 - acc: 0.9846 - val_loss: 1.3481 - val_acc: 0.7160\n",
      "Epoch 21/100\n",
      "22125/22125 [==============================] - 154s 7ms/step - loss: 0.0456 - acc: 0.9837 - val_loss: 1.2949 - val_acc: 0.7251\n",
      "Epoch 22/100\n",
      "22125/22125 [==============================] - 155s 7ms/step - loss: 0.0454 - acc: 0.9831 - val_loss: 1.1797 - val_acc: 0.7162\n",
      "Epoch 23/100\n",
      "22125/22125 [==============================] - 153s 7ms/step - loss: 0.0386 - acc: 0.9855 - val_loss: 1.3759 - val_acc: 0.7207\n",
      "Epoch 24/100\n",
      "22125/22125 [==============================] - 151s 7ms/step - loss: 0.0409 - acc: 0.9851 - val_loss: 1.2871 - val_acc: 0.7113\n",
      "Epoch 25/100\n",
      "22125/22125 [==============================] - 154s 7ms/step - loss: 0.0274 - acc: 0.9901 - val_loss: 1.3936 - val_acc: 0.7220\n",
      "Epoch 26/100\n",
      "22125/22125 [==============================] - 159s 7ms/step - loss: 0.0338 - acc: 0.9879 - val_loss: 1.3570 - val_acc: 0.7249\n",
      "Epoch 27/100\n",
      "22125/22125 [==============================] - 153s 7ms/step - loss: 0.0369 - acc: 0.9865 - val_loss: 1.3338 - val_acc: 0.7294\n",
      "Epoch 28/100\n",
      "22125/22125 [==============================] - 152s 7ms/step - loss: 0.0345 - acc: 0.9873 - val_loss: 1.3736 - val_acc: 0.7267\n",
      "Epoch 29/100\n",
      "22125/22125 [==============================] - 152s 7ms/step - loss: 0.0294 - acc: 0.9894 - val_loss: 1.3201 - val_acc: 0.7411\n",
      "Epoch 30/100\n",
      "22125/22125 [==============================] - 152s 7ms/step - loss: 0.0278 - acc: 0.9897 - val_loss: 1.3246 - val_acc: 0.7381\n",
      "Epoch 31/100\n",
      "22125/22125 [==============================] - 154s 7ms/step - loss: 0.0262 - acc: 0.9902 - val_loss: 1.3882 - val_acc: 0.7256\n",
      "Epoch 32/100\n",
      "22125/22125 [==============================] - 153s 7ms/step - loss: 0.0322 - acc: 0.9882 - val_loss: 1.3662 - val_acc: 0.7336\n",
      "Epoch 33/100\n",
      "22125/22125 [==============================] - 156s 7ms/step - loss: 0.0329 - acc: 0.9878 - val_loss: 1.3312 - val_acc: 0.7326\n",
      "Epoch 34/100\n",
      "20608/22125 [==========================>...] - ETA: 9s - loss: 0.0282 - acc: 0.9895 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4c3859fe2365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                  verbose=1)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "# architecture of the network is adopted from https://arxiv.org/pdf/1511.07035.pdf\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(216, return_sequences=True, activation=\"tanh\",),\n",
    "                        input_shape=(X_train.shape[1:])))\n",
    "model.add(Bidirectional(LSTM(216, return_sequences=True, activation=\"tanh\")))\n",
    "model.add(Bidirectional(LSTM(216, activation=\"tanh\")))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(X_train, y_train,\n",
    "                 # callbacks=[tbCallback, mcCallback, testCallback0, testCallback1, testCallback2, testCallback3],\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 epochs=100,\n",
    "                 batch_size=128,\n",
    "                 verbose=1)\n",
    "\n",
    "\n",
    "print(\"\\nEvaluating...\")\n",
    "y_pred = to_categorical(model.predict_classes(X_test, verbose=1))\n",
    "print(y_pred, y_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy:\", acc)\n",
    "rec = recall_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"Recall:\", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x. References ##\n",
    "\n",
    "1. Gemmeke, Jort F., et al. \"Audio Set: An ontology and human-labeled dataset for audio events.\" *IEEE ICASSP*. 2017.\n",
    "2. AbdiÄ‡, Irman, et al. \"Detecting road surface wetness from audio: A deep learning approach.\" *Pattern Recognition (ICPR), 2016 23rd International Conference on. IEEE*, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
