{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks course project: #\n",
    "# Google AudioSet sound classification with Deep Learning #\n",
    "### Sapienza University of Rome ###\n",
    "\n",
    "### by Ivan Senilov (1787618) ###\n",
    "\n",
    "## 1. Introduction ##\n",
    "\n",
    "This work represents a practical part of the Neural Networks course taught at Sapienza University.\n",
    "\n",
    "The goal of this coursework is to:\n",
    "\n",
    "1. Explore the [Google AudioSet](https://research.google.com/audioset/index.html) [1]\n",
    "2. Build the classification model based on Neural Network(s)\n",
    "3. Validate the model\n",
    "4. Evaluate and discuss the results\n",
    "\n",
    "## 2. Audio classification problem ##\n",
    "\n",
    "### 2.1 Audio features ###\n",
    "\n",
    "Feature extraction is signal processing task of computing the numerical representation from the signal that can be used to characterize the audio segment [3].\n",
    "\n",
    "Most of the audio features fall into three categories [4]:\n",
    "\n",
    "1. **Energy-based**. For example, 4Hz modulation energy used for speech/music classification [5].\n",
    "2. **Spectrum-based**. Examples of the category are roll-off of the spectrum, spectral flux, Mel Frequency Cepstral Coefficents (MFCC) [5] and linear spectrum pair, band periodicity [6].\n",
    "3. **Perceptual based**, like pitch (estimated to discriminate songs and speech over music [7]).\n",
    "\n",
    "The most developed areas of machine learning for audio classification include speech and music recognition where  MFCCs are widely used as features. MFCCs were introduced in 1980 [8] and showed better performance in terms of recognition of spoken words. However, when it comes to other types of sound recognition, the selection of feature extraction method becomes less obvious, even though MFCCs are used in, for example, application to environmental sound classification [9].\n",
    "\n",
    "<img src=\"figs/feat_extr.png\">\n",
    "\n",
    "Extraction pipelines for MPEG-7 (left) and MFCC (right) features (reprinted from [4])\n",
    "\n",
    "Typical approach of feature extraction process is to split the audio signal into small chunks of several $ms$ (exact size is domain-dependent) and feed it into computational function of one of many frameworks for extraction of features from audio (see Figure 1 and 2 for examples of extraction pipelines for MPEG-7 [10] and MFCC features). The most popular frameworks include YAAFE [11] and openSMILE [12], which allows to extract following feature types:\n",
    "\n",
    "1. Amplitude Modulation [13]. Analyzed frequency ranges are: Tremolo (4 - 8 Hz) and Grain (10 - 40 Hz). For each of these ranges, it computes:\n",
    "    * Frequency of maximum energy in range\n",
    "    * Difference of the energy of this frequency and the mean energy over all frequencies\n",
    "    * Difference of the energy of this frequency and the mean energy in range\n",
    "    * Product of the two first values.\n",
    "2. Autocorrelation coefficients $\\mathit{ac}$ on each frame.\n",
    "    $$ac(k)=\\sum_{i=0}^{N-k-1}x(i)x(i+k),$$    \n",
    "    where (here and below) $k$ is frame length in samples, $N$ is length of whole signal in samples and $x(i)$ is signal function.    \n",
    "3. Onset detection using a complex domain spectral flux method [14].    \n",
    "4. Energy $\\mathit{en}$ as root mean square of an audio frame.\n",
    "    $$en=\\sqrt{\\dfrac{\\sum_{i=0}^{N-1}x(i)^2}{N}}$$    \n",
    "5. Envelope of an oscillating signal (smooth curve outlining its extremes).    \n",
    "6. Shape statistics (centroid, spread, skewness and kurtosis) of each frameâ€™s Temporal Shape, Amplitude Envelope and Magnitude Spectrum.    \n",
    "7. Linear Predictor Coefficients (LPC) of a signal frame [15].    \n",
    "8. Line Spectral Frequency (LSF) coefficients of a signal frame [16].    \n",
    "9. Loudness coefficients [17].    \n",
    "10. Mel-frequencies cepstrum coefficients and Mel-frequencies spectrum.    \n",
    "11. Magnitude spectrum.    \n",
    "12. Octave band signal intensity (OBSI) using a triangular octave filter bank and  OBSI ratio between consecutive octave.\n",
    "13. Sharpness and Spread of Loudness coefficients [18].    \n",
    "14. Spectral crest factor per log-spaced band of 1/4 octave.    \n",
    "15. Spectral decrease, spectral flatness.    \n",
    "16. Spectral Flux.    \n",
    "17. Spectral roll-off (frequency so that 99% of the energy is contained below) [5].\n",
    "18. Spectral Slope (computed by linear regression of the spectral amplitude) [18].    \n",
    "19. Spectral Variation (normalized correlation of spectrum between consecutive frames) [18].    \n",
    "20. Zero-crossing rate (ZCR) for frame [5].\n",
    "\n",
    "Even though there are plenty of features available for audio, we focus on MFCCs as they proved to be the most effective representation (See Figure 3 for example heatmap of feature matrix). We also consider raw audio signal to see how automatically learned features (by special Neural Network architecture) perform in comparison with hand-crafted ones.\n",
    "\n",
    "<img src=\"figs/spectr.png\">\n",
    "\n",
    "<p style=\"text-align: center; font-weight:bold\"> Fig.3. Example of heatmap of feature matrix </p>\n",
    "\n",
    "### 2.2 Model selection ###\n",
    "\n",
    "As this work is a part of Neural Networks course, we will not consider more traditional Machine Learning algorithms. Other reason for this is that Neural Networks proved to be much more effective in real world pattern recognition problems, particularly in audio classification like shown by [2] and many other researches.\n",
    "\n",
    "Basically, in the next section we would like to check and find out performance of following combinations of feature extraction techniques and network architectures:\n",
    "\n",
    "1. **MFCC features + Bidirectional Long Short Term Memory (BLSTM) network**, where we use traditional (for audio applications) features with relatively modern Recurrent Neural Network [19,20].\n",
    "2. **Raw audio + Convolutional Neural Network (CNN) and BLSTM network**. CNNs are traditionally used in image recognition [21] where 2Dkernels learn features. We use 1D kernels for learning patterns in raw audio signal instead, keeping the original idea of CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation ##\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Load ontology and correspondence between video's ids and class labels.\n",
    "2. Download the videos from Youtube to respective directories\n",
    "3. Load labeled dataset into Numpy arrays\n",
    "4. Build classifier based on Neural Network architecture\n",
    "5. Evaluate it using cross-validation\n",
    "\n",
    "First of all, import data manipulation libs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify the task (in terms of computational complexity as well) we limit number of categories to 3. Read the ontology to Pandas dataframe and select only *id*, *name* and *child_id* fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_of_interest = (\"Vehicle\", \"Channel, environment and background\", \"Natural sounds\")\n",
    "\n",
    "with open(\"ontology.json\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "\n",
    "ontology = pd.read_json(contents)\n",
    "\n",
    "ontology = ontology[[\"id\", \"name\", \"child_ids\"]].set_index(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we extract *id*s for each (main and child) category (class) of interest recursively, building a dictionary where key is class name and value is a list of *id*s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ids(cls, data):  # recursively add all child classes of input class to the list\n",
    "    out = [] \n",
    "    for id, row in data.iterrows():\n",
    "        if row[\"name\"] == cls:\n",
    "            out.append(id)\n",
    "            if len(row[\"child_ids\"]) > 0:\n",
    "                for child in row[\"child_ids\"]:\n",
    "                    out.append(extract_ids(data[\"name\"][child], data))\n",
    "                \n",
    "    return flattern(out)\n",
    "\n",
    "\n",
    "def flattern(A):  # list flattening helper function\n",
    "    rt = []\n",
    "    for i in A:\n",
    "        if isinstance(i,list): rt.extend(flattern(i))\n",
    "        else: rt.append(i)\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {i:[] for i in classes_of_interest}\n",
    "for cls in classes_of_interest:\n",
    "    classes[cls] = extract_ids(cls, ontology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read tables with correspondance between YouTube ID, segment in the video and ids from ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_seconds</th>\n",
       "      <th>end_seconds</th>\n",
       "      <th>positive_labels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># YTID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>--PJHxphWEs</th>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[/m/09x0r, /t/dd00088]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--ZhevVpy1s</th>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>[/m/012xff]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--aE2O5G5WE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[/m/03fwl, /m/04rlf, /m/09x0r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--aO5cdqSAg</th>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[/t/dd00003, /t/dd00005]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--aaILOrkII</th>\n",
       "      <td>200.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>[/m/032s66, /m/073cg4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             start_seconds  end_seconds                 positive_labels\n",
       "# YTID                                                                 \n",
       "--PJHxphWEs           30.0         40.0          [/m/09x0r, /t/dd00088]\n",
       "--ZhevVpy1s           50.0         60.0                     [/m/012xff]\n",
       "--aE2O5G5WE            0.0         10.0  [/m/03fwl, /m/04rlf, /m/09x0r]\n",
       "--aO5cdqSAg           30.0         40.0        [/t/dd00003, /t/dd00005]\n",
       "--aaILOrkII          200.0        210.0          [/m/032s66, /m/073cg4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"balanced_train_segments.csv\", skiprows=2, sep=\", \", engine=\"python\", index_col=\"# YTID\")\n",
    "train_data[\"positive_labels\"] = train_data[\"positive_labels\"].apply(lambda x: x.replace('\\\"','').split(\",\"))\n",
    "\n",
    "test_data = pd.read_csv(\"eval_segments.csv\", skiprows=2, sep=\", \", engine=\"python\", index_col=\"# YTID\")\n",
    "test_data[\"positive_labels\"] = test_data[\"positive_labels\"].apply(lambda x: x.replace('\\\"','').split(\",\"))\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we download segments of videos from YouTube to respective folders of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtube_dl   # Python library for downloading from YouTube\n",
    "import os\n",
    "\n",
    "\n",
    "'''\n",
    "Helper function for cropping audio with ffmpeg\n",
    "'''\n",
    "def crop(start, length, filename):\n",
    "    command = \"ffmpeg -y -i \" + filename + \\\n",
    "    \" -ss  \" + str(start) + \" -t \" + str(length) + \\\n",
    "    \" -ac 1 -acodec copy \" + filename.split(\".\")[0] + \"_.wav\"\n",
    "    os.system(command)\n",
    "\n",
    "to_download = {i:[] for i in classes_of_interest}\n",
    "    \n",
    "for cls in classes_of_interest:\n",
    "    for id in classes[cls]:\n",
    "        for row in train_data.itertuples(): \n",
    "            if row[3][0] == id:\n",
    "                to_download[cls].append(row[0])\n",
    "                \n",
    "\n",
    "'''\n",
    "options for youtube-dl\n",
    "'''\n",
    "options = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'wav'   # wav format for lossless features extraction\n",
    "    }],\n",
    "    'extractaudio' : True,\n",
    "    'ignoreerrors' : True,\n",
    "    'audioformat' : \"wav\",\n",
    "    'noplaylist' : True,    # only download single clip, not playlist\n",
    "}\n",
    "                \n",
    "for cls in to_download:\n",
    "    # setting path for download removing commas and spaces in order to avoid fylesisitem access problems\n",
    "    options['outtmpl'] = os.path.join(cls.replace(\",\",\"\").replace(\" \",\"\"), '%(id)s.%(ext)s')\n",
    "    for file in to_download[cls]:\n",
    "        with youtube_dl.YoutubeDL(options) as ydl:\n",
    "            ydl.download([file])   # downloading full audio clip (due to limitations of youtube-dl)\n",
    "            filename = os.path.join(cls.replace(\",\",\"\").replace(\" \",\"\"), file + \".wav\")\n",
    "            crop(train_data.loc[file][\"start_seconds\"],\n",
    "                 10, filename)     # cropping of the clip in accordance with dataset csv file \n",
    "            try:\n",
    "                os.remove(filename)   # removing original (non-cropped) clip\n",
    "            except OSError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we extract features with help of SciPy (raw audio) and librosa (for MFCC extraction) libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import wavfile\n",
    "from librosa import feature\n",
    "\n",
    "# Removing commas and spaces from classes names for avoiding problems woth dir names \n",
    "classes_dirs = [i.replace(\",\",\"\").replace(\" \",\"\") for i in classes_of_interest]\n",
    "\n",
    "feat = []\n",
    "feat_raw = []\n",
    "labels = []\n",
    "class_num = 0\n",
    "window = 32768                          # size of window for each sample\n",
    "for directory in classes_dirs:\n",
    "    count = 0\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.path.join(directory, file)\n",
    "        if os.path.isfile(filename) and count < 1000:    # limiting number of files to read\n",
    "            count += 1\n",
    "            rate, frames = wavfile.read(filename)\n",
    "            if len(frames.shape) > 1:    # if stereo, take only one channel\n",
    "                frames = frames[:,0]\n",
    "            for i in range(0, len(frames) - (window+1), int(window/2)):\n",
    "                pxx = feature.mfcc(y=frames[i:i + window], sr=rate, n_mfcc=20)\n",
    "                feat.append(pxx)\n",
    "                feat_raw.append(frames[i:i + window])\n",
    "                labels.append(class_num)\n",
    "    class_num += 1                          # each successive class is represented by incremented integer\n",
    "data = np.stack(feat)\n",
    "data_raw = np.stack(feat_raw)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(labels)             # convert class number to 'one-hot' vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our networks with help of keras deep learning library which provides simple interface for building the nets. But before, we define custom keras callback for evaluating the model after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback#,TensorBoard, ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data, net_type):\n",
    "        self.test_data = test_data\n",
    "        self.net_type = net_type\n",
    "        self.dt = datetime.now().strftime(\"%d-%m-%Y.%H-%M\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        if not os.path.isdir(\"logs\"):\n",
    "            os.mkdir(\"logs\")\n",
    "        log_filename = os.path.join(\"logs\", \"log.\") + self.net_type + \".\" + self.dt + \".csv\"\n",
    "        with open(log_filename, \"a\") as log:\n",
    "            # net type, epoch no, test loss, test acc, train loss, train acc\n",
    "            log.write(\"{},{},{},{},{}\\n\".format(epoch, loss, acc, logs[\"loss\"], logs[\"acc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we try 3-layers BLSTM network adopted from [2] as it has proved its effectiveness in similar problem. During the experiments it was found out that ReLU activation function in RNN layers results in model stop learning which is most probably consequence of vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Conv2D, Dense, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "# architecture of the network is adopted from https://arxiv.org/pdf/1511.07035.pdf\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(216, return_sequences=True, activation=\"tanh\", dropout=0.5),\n",
    "                        input_shape=(X_train.shape[1:])))\n",
    "model.add(Bidirectional(LSTM(216, return_sequences=True, activation=\"tanh\", dropout=0.4)))\n",
    "model.add(Bidirectional(LSTM(216, activation=\"tanh\", dropout=0.3)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "test_callback = TestCallback((X_test, y_test), \"MFCC-BLSTM-drop\")\n",
    "\n",
    "hist = model.fit(X_train, y_train,\n",
    "                 callbacks= [test_callback],\n",
    "                 # validation_data=(X_test, y_test),\n",
    "                 epochs=100,\n",
    "                 batch_size=128,\n",
    "                 verbose=1)\n",
    "\n",
    "print(\"\\nEvaluating...\")\n",
    "y_pred = to_categorical(model.predict_classes(X_test, verbose=1))\n",
    "print(y_pred, y_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy:\", acc)\n",
    "rec = recall_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"Recall:\", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the network is depicted on the Figure 4 \n",
    "\n",
    "<img src=\"figs/blstm.png\">\n",
    "<p style=\"text-align: center; font-weight:bold\"> Fig.4. Architecture of the BLSTM network trained on MFCC features</p>\n",
    "\n",
    "\n",
    "The above code already includes regularization (dropout) but first experiment was performed without it. Let's compare the performance of the model with and without dropout (Figures 5 and 6 respectively).\n",
    "\n",
    "<img src=\"figs/mfcc-blstm.png\">\n",
    "<p style=\"text-align: center; font-weight:bold\"> Fig.5. Accuracy graphs of BLSTM network on MFCC features (w/o dropout)</p>\n",
    "\n",
    "The clear evidence of overfitting is on the graph where training accuracy reaches 98% but test set accuracy is only 73% at its maximum point.\n",
    "\n",
    "<img src=\"figs/mfcc-blstm-drop.png\">\n",
    "<p style=\"text-align: center; font-weight:bold\"> Fig.6. Accuracy graphs of BLSTM network on MFCC features (w/dropout) </p>\n",
    "\n",
    "Apparently, dropout increases test set (unseen by the model) accuracy so we will add it to all consequent models regardless of their architecture.\n",
    "\n",
    "Next, we add convolutional layers in the beginning of the network and test on the same features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Conv1D, Dense\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import MaxPooling1D, Dropout, GlobalAveragePooling1D\n",
    "\n",
    "\n",
    "data_raw_1 = data_raw[:,::10]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_raw_1, labels, test_size=0.2, random_state=42)\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, int(X_train.shape[2])))\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, int(X_test.shape[2])))\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "print(\"\\nShape of the dataset:\", X_train.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=64, strides=1, padding=\"same\", activation='tanh'),\n",
    "                          input_shape=(1, 8192, 1)))\n",
    "model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "model.add(TimeDistributed(Dropout(0.4)))\n",
    "model.add(TimeDistributed(Conv1D(64, 64, padding=\"same\", activation='tanh')))\n",
    "model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "model.add(TimeDistributed(Dropout(0.4)))\n",
    "model.add(TimeDistributed(Conv1D(128, 64, padding=\"same\", activation='tanh')))\n",
    "model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "model.add(TimeDistributed(Dropout(0.4)))\n",
    "model.add(TimeDistributed(Conv1D(128, 64, padding=\"same\", activation='tanh')))\n",
    "model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "# model.add(TimeDistributed(Dropout(0.4)))\n",
    "model.add(TimeDistributed(Conv1D(256, 64, padding=\"same\", activation='tanh')))\n",
    "model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "# model.add(TimeDistributed(Dropout(0.4)))\n",
    "model.add(TimeDistributed(Conv1D(256, 32, padding=\"same\", activation='tanh')))\n",
    "model.add(TimeDistributed(GlobalAveragePooling1D()))\n",
    "model.add(Bidirectional(LSTM(216, return_sequences=True, activation=\"tanh\", dropout=0.3)))\n",
    "model.add(Bidirectional(LSTM(216, return_sequences=True, activation=\"tanh\", dropout=0.2)))\n",
    "model.add(Bidirectional(LSTM(216, activation=\"tanh\")))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "test_callback = TestCallback((X_test, y_test), \"MFCC-CNN-BLSTM-drop\")\n",
    "\n",
    "hist = model.fit(X_train, y_train,\n",
    "                 callbacks=[test_callback],\n",
    "                 # validation_data=(X_1, y_1),\n",
    "                 epochs=100,\n",
    "                 batch_size=128,\n",
    "                 verbose=1)\n",
    "\n",
    "\n",
    "print(\"\\nEvaluating...\")\n",
    "y_pred = to_categorical(model.predict_classes(X_test, verbose=1))\n",
    "print(y_pred, y_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy:\", acc)\n",
    "rec = recall_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"Recall:\", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the network is depicted on the Figure 7\n",
    "\n",
    "<img src=\"figs/cnn-blstm.png\">\n",
    "<p style=\"text-align: center; font-weight:bold\"> Fig.7. Architecture of the CNN-BLSTM network trained on raw audio</p>\n",
    "\n",
    "This architecture is much more complicated that initial one and training one epoch on 1,000 snippets **takes about 1,400s on Core i7 + GTX 1080Ti** with TensorFlow-GPU.\n",
    "\n",
    "Now let's see at the performance of the model on Figure 8:\n",
    "\n",
    "<img src=\"figs/raw-cnn-blstm-drop.png\">\n",
    "<p style=\"text-align: center; font-weight:bold\"> Fig.8. Accuracy graphs of CNN-BLSTM network on raw audio (w/dropout)</p>\n",
    "\n",
    "Here, in opposite to previous case, we see underfitting which is probably caused by not optimal selection of window length or hyperparameters of the network (too strong regularization). Unfortunately, I couldn't test other parameters due to limited access to GPU powered workstation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion, conclusion and further steps ##\n",
    "\n",
    "In this work, I explored the problem of audio classification on the example of Google's AudioSet. Firstly, the existing approaches to features extraction and model building were reviewed. Then the hypothesis of application of CNN to raw audio was checked by implementation of the proposed architectures. We saw that 3-layer BLSTM network on MFCC features yields good results even though it is prone to overfitting. On the other hand, adding CNN layers in the beginning of the network complicates the network significantly but doesn't increase performance (however it doesn't overfit much).\n",
    "\n",
    "Next steps could include extensive testing the different hyperparameters for both implemented networks (varying number and type of layers, number of neurons/filters, size of the filters, regularization, etc.) but it was not done due to limited computational resources and time. The same reason is responsible for absence of cross validation as it would increase training time by k times in case of k-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References ##\n",
    "\n",
    "1. Gemmeke, Jort F., et al. \"Audio Set: An ontology and human-labeled dataset for audio events.\" *IEEE ICASSP*. 2017.\n",
    "2. AbdiÄ‡, Irman, et al. \"Detecting road surface wetness from audio: A deep learning approach.\" *Pattern Recognition (ICPR), 2016 23rd International Conference on. IEEE*, 2016.\n",
    "3. Prasad, Bhanu, and SR Mahadeva Prasanna, eds. Speech, audio, image and biomedical signal processing using neural networks. Vol. 83. Springer, 2007.\n",
    "4. Xiong, Ziyou, et al. \"Comparing MFCC and MPEG-7 audio features for feature extraction, maximum likelihood HMM and entropic prior HMM for sports audio classification.\" Multimedia and Expo, 2003. ICME'03. Proceedings. 2003 International Conference on. Vol. 3. IEEE, 2003.\n",
    "5. Scheirer, Eric, and Malcolm Slaney. \"Construction and evaluation of a robust multifeature speech/music discriminator.\" Acoustics, Speech, and Signal Processing, 1997. ICASSP-97., 1997 IEEE International Conference on. Vol. 2. IEEE, 1997.\n",
    "6. Lu, Lie, Stan Z. Li, and Hong-Jiang Zhang. \"Content-based audio segmentation using support vector machines.\" Proceedings of the IEEE International Conference on Multimedia and Expo (ICME 2001). 2001.\n",
    "7. Zhang, Tong, and C-C. Jay Kuo. \"Content-based classification and retrieval of audio.\" Advanced Signal Processing Algorithms, Architectures, and Implementations VIII. Vol. 3461. International Society for Optics and Photonics, 1998.\n",
    "8. Davis, Steven, and Paul Mermelstein. \"Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences.\" IEEE transactions on acoustics, speech, and signal processing 28.4 (1980): 357-366.\n",
    "9. Chu, Selina, Shrikanth Narayanan, and C-C. Jay Kuo. \"Environmental sound recognition with timeâ€“frequency audio features.\" IEEE Transactions on Audio, Speech, and Language Processing 17.6 (2009): 1142-1158.\n",
    "10. Casey, Michael. \"MPEG-7 sound-recognition tools.\" IEEE Transactions on circuits and Systems for video Technology 11.6 (2001): 737-747.\n",
    "11. Mathieu, Benoit, et al. \"YAAFE, an Easy to Use and Efficient Audio Feature Extraction Software.\" ISMIR. 2010.\n",
    "12. Eyben, Florian, Martin WÃ¶llmer, and BjÃ¶rn Schuller. \"Opensmile: the munich versatile and fast open-source audio feature extractor.\" Proceedings of the 18th ACM international conference on Multimedia. ACM, 2010.\n",
    "13. Eronen, Antti. \"Automatic musical instrument recognition.\" MÃ©moire de DEA, Tempere University of Technology (2001): 178.\n",
    "14. Duxbury, Chris, et al. \"Complex domain onset detection for musical signals.\" Proc. Digital Audio Effects Workshop (DAFx). Vol. 1. London: Queen Mary University, 2003.\n",
    "15. Makhoul, John. \"Linear prediction: A tutorial review.\" Proceedings of the IEEE 63.4 (1975): 561-580.\n",
    "16. BÃ¤ckstrÃ¶m, Tom, and Carlo Magi. \"Properties of line spectrum pair polynomialsâ€”A review.\" Signal processing 86.11 (2006): 3286-3298.\n",
    "17. Moore, Brian CJ, Brian R. Glasberg, and Thomas Baer. \"A model for the prediction of thresholds, loudness, and partial loudness.\" Journal of the Audio Engineering Society 45.4 (1997): 224-240.\n",
    "18. Peeters, Geoffroy. \"A large set of audio features for sound description (similarity and classification) in the CUIDADO project.\" (2004).\n",
    "19. Hochreiter, Sepp, and JÃ¼rgen Schmidhuber. \"Long short-term memory.\" Neural computation 9.8 (1997): 1735-1780.\n",
    "20. Eyben, Florian, et al. \"Universal onset detection with bidirectional long-short term memory neural networks.\" Proc. 11th Intern. Soc. for Music Information Retrieval Conference, ISMIR, Utrecht, The Netherlands. 2010.\n",
    "21. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
