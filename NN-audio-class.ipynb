{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks course project: #\n",
    "# Google AudioSet sound classification with Deep Learning #\n",
    "### Sapienza University of Rome ###\n",
    "\n",
    "### by Ivan Senilov (1787618) ###\n",
    "\n",
    "## 1. Introduction ##\n",
    "\n",
    "This work represents a practical part of the Neural Networks course taught at Sapienza University.\n",
    "\n",
    "The goal of this coursework is to:\n",
    "\n",
    "1. Explore the [Google AudioSet](https://research.google.com/audioset/index.html) [1]\n",
    "2. Build the classification model based on Neural Network(s)\n",
    "3. Validate the model\n",
    "4. Evaluate and discuss the results\n",
    "\n",
    "## 2. Audio classification problem ##\n",
    "\n",
    "### 2.1 Audio features ###\n",
    "\n",
    "Feature extraction is signal processing task of computing the numerical representation from the signal that can be used to characterize the audio segment [3].\n",
    "\n",
    "Most of the audio features fall into three categories [4]:\n",
    "\n",
    "1. **Energy-based**. For example, 4Hz modulation energy used for speech/music classification [5].\n",
    "2. **Spectrum-based**. Examples of the category are roll-off of the spectrum, spectral flux, Mel Frequency Cepstral Coefficents (MFCC) [5] and linear spectrum pair, band periodicity [6].\n",
    "3. **Perceptual based**, like pitch (estimated to discriminate songs and speech over music [7]).\n",
    "\n",
    "The most developed areas of machine learning for audio classification include speech and music recognition where  MFCCs are widely used as features. MFCCs were introduced in 1980 [8] and showed better performance in terms of recognition of spoken words. However, when it comes to other types of sound recognition, the selection of feature extraction method becomes less obvious, even though MFCCs are used in, for example, application to environmental sound classification [9].\n",
    "\n",
    "<img src=\"figs/feat_extr.png\">\n",
    "\n",
    "Extraction pipelines for MPEG-7 (left) and MFCC (right) features (reprinted from [4])\n",
    "\n",
    "Typical approach of feature extraction process is to split the audio signal into small chunks of several $ms$ (exact size is domain-dependent) and feed it into computational function of one of many frameworks for extraction of features from audio (see Figure 1 and 2 for examples of extraction pipelines for MPEG-7 [10] and MFCC features). The most popular frameworks include YAAFE [11] and openSMILE [12], which allows to extract following feature types:\n",
    "\n",
    "1. Amplitude Modulation [13]. Analyzed frequency ranges are: Tremolo (4 - 8 Hz) and Grain (10 - 40 Hz). For each of these ranges, it computes:\n",
    "    * Frequency of maximum energy in range\n",
    "    * Difference of the energy of this frequency and the mean energy over all frequencies\n",
    "    * Difference of the energy of this frequency and the mean energy in range\n",
    "    * Product of the two first values.\n",
    "2. Autocorrelation coefficients $\\mathit{ac}$ on each frame.\n",
    "    $$ac(k)=\\sum_{i=0}^{N-k-1}x(i)x(i+k),$$    \n",
    "    where (here and below) $k$ is frame length in samples, $N$ is length of whole signal in samples and $x(i)$ is signal function.    \n",
    "3. Onset detection using a complex domain spectral flux method [14].    \n",
    "4. Energy $\\mathit{en}$ as root mean square of an audio frame.\n",
    "    $$en=\\sqrt{\\dfrac{\\sum_{i=0}^{N-1}x(i)^2}{N}}$$    \n",
    "5. Envelope of an oscillating signal (smooth curve outlining its extremes).    \n",
    "6. Shape statistics (centroid, spread, skewness and kurtosis) of each frameâ€™s Temporal Shape, Amplitude Envelope and Magnitude Spectrum.    \n",
    "7. Linear Predictor Coefficients (LPC) of a signal frame [15].    \n",
    "8. Line Spectral Frequency (LSF) coefficients of a signal frame [16].    \n",
    "9. Loudness coefficients [17].    \n",
    "10. Mel-frequencies cepstrum coefficients and Mel-frequencies spectrum.    \n",
    "11. Magnitude spectrum.    \n",
    "12. Octave band signal intensity (OBSI) using a triangular octave filter bank and  OBSI ratio between consecutive octave.\n",
    "13. Sharpness and Spread of Loudness coefficients [18].    \n",
    "14. Spectral crest factor per log-spaced band of 1/4 octave.    \n",
    "15. Spectral decrease, spectral flatness.    \n",
    "16. Spectral Flux.    \n",
    "17. Spectral roll-off (frequency so that 99% of the energy is contained below) [5].\n",
    "18. Spectral Slope (computed by linear regression of the spectral amplitude) [18].    \n",
    "19. Spectral Variation (normalized correlation of spectrum between consecutive frames) [18].    \n",
    "20. Zero-crossing rate (ZCR) for frame [5].\n",
    "\n",
    "Even though there are plenty of features available for audio, we focus on MFCCs as they proved to be the most effective representation (See Figure 3 for example heatmap of feature matrix). We also consider raw audio signal to see how automatically learned features (by special Neural Network architecture) perform in comparison with hand-crafted ones.\n",
    "\n",
    "<img src=\"figs/spectr.png\">\n",
    "\n",
    "<p style=\"text-align: center; font-weight:bold\"> Fig.3. Example of heatmap of feature matrix </p>\n",
    "\n",
    "### 2.2 Model selection ###\n",
    "\n",
    "As this work is a part of Neural Networks course, we will not consider more traditional Machine Learning algorithms. Other reason for this is that Neural Networks proved to be much more effective in real world pattern recogntion problems, particularly in audio classification like shown by [2] and many other researches.\n",
    "\n",
    "Basically, in the next section we would like to check and find out performance of following combinations of feature extraction techniques and network architectures:\n",
    "\n",
    "1. **MFCC features + Bidirectional Long Short Term Memory (BLSTM) network**, where we use traditional (for audio applications) features with relatively modern Recurrent Neural Network [19,20].\n",
    "2. **Raw audio + Convolutional Neural Network (CNN) and BLSTM network**. CNNs are traditionally used in image recognition [21] where 2Dkernels learn features. We use 1D kernels for learning patterns in raw audio signal instead, keeping the original idea of CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation ##\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Load ontology and correspondence between video's ids and class labels.\n",
    "2. Download the videos from Youtube to respective directories\n",
    "3. Load labeled dataset into Numpy arrays\n",
    "4. Build classifier based on Neural Network architecture\n",
    "5. Evaluate it using cross-validation\n",
    "\n",
    "First of all, import data manipulation libs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify the task (in terms of computational complexity as well) we limit number of categories to 3. Read the ontology to Pandas dataframe and select only *id*, *name* and *child_id* fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_of_interest = (\"Vehicle\", \"Channel, environment and background\", \"Natural sounds\")\n",
    "\n",
    "with open(\"ontology.json\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "\n",
    "ontology = pd.read_json(contents)\n",
    "\n",
    "ontology = ontology[[\"id\", \"name\", \"child_ids\"]].set_index(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we extract *id*s for each (main and child) category (class) of interest recursively, building a dictionary where key is class name and value is a list of *id*s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ids(cls, data):  # recursively add all child classes of input class to the list\n",
    "    out = [] \n",
    "    for id, row in data.iterrows():\n",
    "        if row[\"name\"] == cls:\n",
    "            out.append(id)\n",
    "            if len(row[\"child_ids\"]) > 0:\n",
    "                for child in row[\"child_ids\"]:\n",
    "                    out.append(extract_ids(data[\"name\"][child], data))\n",
    "                \n",
    "    return flattern(out)\n",
    "\n",
    "\n",
    "def flattern(A):  # list flattening helper function\n",
    "    rt = []\n",
    "    for i in A:\n",
    "        if isinstance(i,list): rt.extend(flattern(i))\n",
    "        else: rt.append(i)\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {i:[] for i in classes_of_interest}\n",
    "for cls in classes_of_interest:\n",
    "    classes[cls] = extract_ids(cls, ontology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read tables with correspondance between YouTube ID, segment in the video and ids from ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_seconds</th>\n",
       "      <th>end_seconds</th>\n",
       "      <th>positive_labels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># YTID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>--PJHxphWEs</th>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[/m/09x0r, /t/dd00088]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--ZhevVpy1s</th>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>[/m/012xff]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--aE2O5G5WE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[/m/03fwl, /m/04rlf, /m/09x0r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--aO5cdqSAg</th>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[/t/dd00003, /t/dd00005]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--aaILOrkII</th>\n",
       "      <td>200.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>[/m/032s66, /m/073cg4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             start_seconds  end_seconds                 positive_labels\n",
       "# YTID                                                                 \n",
       "--PJHxphWEs           30.0         40.0          [/m/09x0r, /t/dd00088]\n",
       "--ZhevVpy1s           50.0         60.0                     [/m/012xff]\n",
       "--aE2O5G5WE            0.0         10.0  [/m/03fwl, /m/04rlf, /m/09x0r]\n",
       "--aO5cdqSAg           30.0         40.0        [/t/dd00003, /t/dd00005]\n",
       "--aaILOrkII          200.0        210.0          [/m/032s66, /m/073cg4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"balanced_train_segments.csv\", skiprows=2, sep=\", \", engine=\"python\", index_col=\"# YTID\")\n",
    "train_data[\"positive_labels\"] = train_data[\"positive_labels\"].apply(lambda x: x.replace('\\\"','').split(\",\"))\n",
    "\n",
    "test_data = pd.read_csv(\"eval_segments.csv\", skiprows=2, sep=\", \", engine=\"python\", index_col=\"# YTID\")\n",
    "test_data[\"positive_labels\"] = test_data[\"positive_labels\"].apply(lambda x: x.replace('\\\"','').split(\",\"))\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we download segments of videos from YouTube to respective folders of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtube_dl   # Python library for downloading from YouTube\n",
    "import os\n",
    "\n",
    "\n",
    "'''\n",
    "Helper function for cropping audio with ffmpeg\n",
    "'''\n",
    "def crop(start, length, filename):\n",
    "    command = \"ffmpeg -y -i \" + filename + \\\n",
    "    \" -ss  \" + str(start) + \" -t \" + str(length) + \\\n",
    "    \" -ac 1 -acodec copy \" + filename.split(\".\")[0] + \"_.wav\"\n",
    "    os.system(command)\n",
    "\n",
    "to_download = {i:[] for i in classes_of_interest}\n",
    "    \n",
    "for cls in classes_of_interest:\n",
    "    for id in classes[cls]:\n",
    "        for row in train_data.itertuples(): \n",
    "            if row[3][0] == id:\n",
    "                to_download[cls].append(row[0])\n",
    "                \n",
    "\n",
    "'''\n",
    "options for youtube-dl\n",
    "'''\n",
    "options = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'wav'   # wav format for lossless features extraction\n",
    "    }],\n",
    "    'extractaudio' : True,\n",
    "    'ignoreerrors' : True,\n",
    "    'audioformat' : \"wav\",\n",
    "    'noplaylist' : True,    # only download single clip, not playlist\n",
    "}\n",
    "                \n",
    "for cls in to_download:\n",
    "    # setting path for download removing commas and spaces in order to avoid fylesisitem access problems\n",
    "    options['outtmpl'] = os.path.join(cls.replace(\",\",\"\").replace(\" \",\"\"), '%(id)s.%(ext)s')\n",
    "    for file in to_download[cls]:\n",
    "        with youtube_dl.YoutubeDL(options) as ydl:\n",
    "            ydl.download([file])   # downloading full audio clip (due to limitations of youtube-dl)\n",
    "            filename = os.path.join(cls.replace(\",\",\"\").replace(\" \",\"\"), file + \".wav\")\n",
    "            crop(train_data.loc[file][\"start_seconds\"],\n",
    "                 10, filename)     # cropping of the clip in accordance with dataset csv file \n",
    "            try:\n",
    "                os.remove(filename)   # removing original (non-cropped) clip\n",
    "            except OSError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we extract features with help of SciPy (raw audio) and librosa (for MFCC extraction) libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/isenilov/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy.io import wavfile\n",
    "from librosa import feature\n",
    "\n",
    "# Removing commas and spaces from classes names for avoiding problems woth dir names \n",
    "classes_dirs = [i.replace(\",\",\"\").replace(\" \",\"\") for i in classes_of_interest]\n",
    "\n",
    "feat = []\n",
    "feat_raw = []\n",
    "labels = []\n",
    "class_num = 0\n",
    "window = 32768                          # size of window for each sample\n",
    "for directory in classes_dirs:\n",
    "    count = 0\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.path.join(directory, file)\n",
    "        if os.path.isfile(filename) and count < 1000:    # limiting number of files to read\n",
    "            count += 1\n",
    "            rate, frames = wavfile.read(filename)\n",
    "            if len(frames.shape) > 1:    # if stereo, take only one channel\n",
    "                frames = frames[:,0]\n",
    "            for i in range(0, len(frames) - (window+1), int(window/2)):\n",
    "                pxx = feature.mfcc(y=frames[i:i + window], sr=rate, n_mfcc=20)\n",
    "                feat.append(pxx)\n",
    "                feat_raw.append(frames[i:i + window])\n",
    "                labels.append(class_num)\n",
    "    class_num += 1                          # each successive class is represented by incremented integer\n",
    "data = np.stack(feat)\n",
    "data_raw = np.stack(feat_raw)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(labels)             # convert class number to 'one-hot' vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our netwroks with help of keras deep learning library which provides simple interface for building the nets. But before, we define custom keras callback for evaluating the model after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback#,TensorBoard, ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data, net_type):\n",
    "        self.test_data = test_data\n",
    "        self.net_type = net_type\n",
    "        self.dt = datetime.now().strftime(\"%d-%m-%Y.%H-%M\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        if not os.path.isdir(\"logs\"):\n",
    "            os.mkdir(\"logs\")\n",
    "        log_filename = os.path.join(\"logs\", \"log.\") + self.net_type + \".\" + self.dt + \".csv\"\n",
    "        with open(log_filename, \"a\") as log:\n",
    "            # net type, epoch no, test loss, test acc, train loss, train acc\n",
    "            log.write(\"{},{},{},{},{}\\n\".format(epoch, loss, acc, logs[\"loss\"], logs[\"acc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we try 3-layers BLSTM network adopted from [2] as it has proved its effectiveness in similar problem. During the experiments it was found out that ReLU activation function in RNN layers results in model stop learning which is most propably consequence of vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 20, 432)           487296    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 20, 432)           1121472   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 432)               1121472   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 1299      \n",
      "=================================================================\n",
      "Total params: 2,731,539\n",
      "Trainable params: 2,731,539\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "43053/43053 [==============================] - 308s 7ms/step - loss: 1.0302 - acc: 0.4945\n",
      "Epoch 2/100\n",
      "43053/43053 [==============================] - 305s 7ms/step - loss: 0.9956 - acc: 0.5155\n",
      "Epoch 3/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.9667 - acc: 0.5324\n",
      "Epoch 4/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.9430 - acc: 0.5480\n",
      "Epoch 5/100\n",
      "43053/43053 [==============================] - 304s 7ms/step - loss: 0.9173 - acc: 0.5654\n",
      "Epoch 6/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.8855 - acc: 0.5828\n",
      "Epoch 7/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.8348 - acc: 0.6134\n",
      "Epoch 8/100\n",
      "43053/43053 [==============================] - 304s 7ms/step - loss: 0.7709 - acc: 0.6534\n",
      "Epoch 9/100\n",
      "43053/43053 [==============================] - 304s 7ms/step - loss: 0.7042 - acc: 0.6924\n",
      "Epoch 10/100\n",
      "43053/43053 [==============================] - 304s 7ms/step - loss: 0.6439 - acc: 0.7236\n",
      "Epoch 11/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.5795 - acc: 0.7541\n",
      "Epoch 12/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.5313 - acc: 0.7791\n",
      "Epoch 13/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.4871 - acc: 0.7982\n",
      "Epoch 14/100\n",
      "43053/43053 [==============================] - 304s 7ms/step - loss: 0.4520 - acc: 0.8119\n",
      "Epoch 15/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.4174 - acc: 0.8318\n",
      "Epoch 16/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.3862 - acc: 0.8454\n",
      "Epoch 17/100\n",
      "43053/43053 [==============================] - 305s 7ms/step - loss: 0.3620 - acc: 0.8541\n",
      "Epoch 18/100\n",
      "43053/43053 [==============================] - 306s 7ms/step - loss: 0.3339 - acc: 0.8649\n",
      "Epoch 19/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.3138 - acc: 0.8729\n",
      "Epoch 20/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.2931 - acc: 0.8837\n",
      "Epoch 21/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.2773 - acc: 0.8907\n",
      "Epoch 22/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.2641 - acc: 0.8957\n",
      "Epoch 23/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.2464 - acc: 0.9040\n",
      "Epoch 24/100\n",
      "43053/43053 [==============================] - 304s 7ms/step - loss: 0.2417 - acc: 0.9042\n",
      "Epoch 25/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.2292 - acc: 0.9108\n",
      "Epoch 26/100\n",
      "43053/43053 [==============================] - 304s 7ms/step - loss: 0.2144 - acc: 0.9166\n",
      "Epoch 27/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.2095 - acc: 0.9172\n",
      "Epoch 28/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.2019 - acc: 0.9205\n",
      "Epoch 29/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.1948 - acc: 0.9246\n",
      "Epoch 30/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.1886 - acc: 0.9260\n",
      "Epoch 31/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.1794 - acc: 0.9314\n",
      "Epoch 32/100\n",
      "43053/43053 [==============================] - 304s 7ms/step - loss: 0.1675 - acc: 0.9347\n",
      "Epoch 33/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.1655 - acc: 0.9363\n",
      "Epoch 34/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.1618 - acc: 0.9378\n",
      "Epoch 35/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1606 - acc: 0.9378\n",
      "Epoch 36/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1540 - acc: 0.9397\n",
      "Epoch 37/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1512 - acc: 0.9417\n",
      "Epoch 38/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1494 - acc: 0.9430\n",
      "Epoch 39/100\n",
      "43053/43053 [==============================] - 300s 7ms/step - loss: 0.1408 - acc: 0.9456\n",
      "Epoch 40/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1401 - acc: 0.9467\n",
      "Epoch 41/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1375 - acc: 0.9467\n",
      "Epoch 42/100\n",
      "43053/43053 [==============================] - 304s 7ms/step - loss: 0.1353 - acc: 0.9487\n",
      "Epoch 43/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.1355 - acc: 0.9479\n",
      "Epoch 44/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.1321 - acc: 0.9503\n",
      "Epoch 45/100\n",
      "43053/43053 [==============================] - 300s 7ms/step - loss: 0.1324 - acc: 0.9489\n",
      "Epoch 46/100\n",
      "43053/43053 [==============================] - 304s 7ms/step - loss: 0.1293 - acc: 0.9513\n",
      "Epoch 47/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.1297 - acc: 0.9506\n",
      "Epoch 48/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1252 - acc: 0.9527\n",
      "Epoch 49/100\n",
      "43053/43053 [==============================] - 299s 7ms/step - loss: 0.1220 - acc: 0.9549\n",
      "Epoch 50/100\n",
      "43053/43053 [==============================] - 306s 7ms/step - loss: 0.1251 - acc: 0.9525\n",
      "Epoch 51/100\n",
      "43053/43053 [==============================] - 308s 7ms/step - loss: 0.1228 - acc: 0.9538\n",
      "Epoch 52/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1150 - acc: 0.9559\n",
      "Epoch 53/100\n",
      "43053/43053 [==============================] - 304s 7ms/step - loss: 0.1179 - acc: 0.9548\n",
      "Epoch 54/100\n",
      "43053/43053 [==============================] - 305s 7ms/step - loss: 0.1118 - acc: 0.9567\n",
      "Epoch 55/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1185 - acc: 0.9556\n",
      "Epoch 56/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.1151 - acc: 0.9563\n",
      "Epoch 57/100\n",
      "43053/43053 [==============================] - 305s 7ms/step - loss: 0.1090 - acc: 0.9580\n",
      "Epoch 58/100\n",
      "43053/43053 [==============================] - 304s 7ms/step - loss: 0.1168 - acc: 0.9562\n",
      "Epoch 59/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1145 - acc: 0.9563\n",
      "Epoch 60/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1153 - acc: 0.9575\n",
      "Epoch 61/100\n",
      "43053/43053 [==============================] - 300s 7ms/step - loss: 0.1076 - acc: 0.9589\n",
      "Epoch 62/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1086 - acc: 0.9597\n",
      "Epoch 63/100\n",
      "43053/43053 [==============================] - 305s 7ms/step - loss: 0.1053 - acc: 0.9601\n",
      "Epoch 64/100\n",
      "43053/43053 [==============================] - 305s 7ms/step - loss: 0.1066 - acc: 0.9595\n",
      "Epoch 65/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1107 - acc: 0.9575\n",
      "Epoch 66/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.1064 - acc: 0.9601\n",
      "Epoch 67/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.1072 - acc: 0.9595\n",
      "Epoch 68/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.1051 - acc: 0.9605\n",
      "Epoch 69/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.1029 - acc: 0.9612\n",
      "Epoch 70/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.1075 - acc: 0.9603\n",
      "Epoch 71/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.1080 - acc: 0.9583\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43053/43053 [==============================] - 304s 7ms/step - loss: 0.1050 - acc: 0.9592\n",
      "Epoch 73/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.1049 - acc: 0.9607\n",
      "Epoch 74/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.1074 - acc: 0.9594\n",
      "Epoch 75/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.0980 - acc: 0.9638\n",
      "Epoch 76/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.0978 - acc: 0.9627\n",
      "Epoch 77/100\n",
      "43053/43053 [==============================] - 299s 7ms/step - loss: 0.0974 - acc: 0.9635\n",
      "Epoch 78/100\n",
      "43053/43053 [==============================] - 300s 7ms/step - loss: 0.0967 - acc: 0.9639\n",
      "Epoch 79/100\n",
      "43053/43053 [==============================] - 300s 7ms/step - loss: 0.0999 - acc: 0.9617\n",
      "Epoch 80/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.0961 - acc: 0.9636\n",
      "Epoch 81/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.0942 - acc: 0.9640\n",
      "Epoch 82/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.0971 - acc: 0.9640\n",
      "Epoch 83/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.1012 - acc: 0.9618\n",
      "Epoch 84/100\n",
      "43053/43053 [==============================] - 300s 7ms/step - loss: 0.0998 - acc: 0.9621\n",
      "Epoch 85/100\n",
      "43053/43053 [==============================] - 300s 7ms/step - loss: 0.0984 - acc: 0.9632\n",
      "Epoch 86/100\n",
      "43053/43053 [==============================] - 300s 7ms/step - loss: 0.0983 - acc: 0.9630\n",
      "Epoch 87/100\n",
      "43053/43053 [==============================] - 300s 7ms/step - loss: 0.0964 - acc: 0.9638\n",
      "Epoch 88/100\n",
      "43053/43053 [==============================] - 299s 7ms/step - loss: 0.0992 - acc: 0.9629\n",
      "Epoch 89/100\n",
      "43053/43053 [==============================] - 300s 7ms/step - loss: 0.0977 - acc: 0.9638\n",
      "Epoch 90/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.0952 - acc: 0.9644\n",
      "Epoch 91/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.0923 - acc: 0.9647\n",
      "Epoch 92/100\n",
      "43053/43053 [==============================] - 303s 7ms/step - loss: 0.0973 - acc: 0.9630\n",
      "Epoch 93/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.0904 - acc: 0.9660\n",
      "Epoch 94/100\n",
      "43053/43053 [==============================] - 301s 7ms/step - loss: 0.0983 - acc: 0.9638\n",
      "Epoch 95/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.0981 - acc: 0.9634\n",
      "Epoch 96/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.0912 - acc: 0.9655\n",
      "Epoch 97/100\n",
      "43053/43053 [==============================] - 299s 7ms/step - loss: 0.0958 - acc: 0.9636\n",
      "Epoch 98/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.0964 - acc: 0.9630\n",
      "Epoch 99/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.0957 - acc: 0.9640\n",
      "Epoch 100/100\n",
      "43053/43053 [==============================] - 302s 7ms/step - loss: 0.0932 - acc: 0.9645\n",
      "\n",
      "Evaluating...\n",
      "10764/10764 [==============================] - 30s 3ms/step\n",
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]] [[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "\n",
      "Accuracy: 0.857302118172\n",
      "Recall: 0.83675553699\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Conv2D, Dense, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "# architecture of the network is adopted from https://arxiv.org/pdf/1511.07035.pdf\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(216, return_sequences=True, activation=\"tanh\", dropout=0.5),\n",
    "                        input_shape=(X_train.shape[1:])))\n",
    "model.add(Bidirectional(LSTM(216, return_sequences=True, activation=\"tanh\", dropout=0.4)))\n",
    "model.add(Bidirectional(LSTM(216, activation=\"tanh\", dropout=0.3)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "test_callback = TestCallback((X_test, y_test), \"MFCC-BLSTM-drop\")\n",
    "\n",
    "hist = model.fit(X_train, y_train,\n",
    "                 callbacks= [test_callback],\n",
    "                 # validation_data=(X_test, y_test),\n",
    "                 epochs=100,\n",
    "                 batch_size=128,\n",
    "                 verbose=1)\n",
    "\n",
    "print(\"\\nEvaluating...\")\n",
    "y_pred = to_categorical(model.predict_classes(X_test, verbose=1))\n",
    "print(y_pred, y_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy:\", acc)\n",
    "rec = recall_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"Recall:\", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code already includes regularization (dropout) but first experiment was performed without it. Let's compare the performance of the model with and without dropout (Figures 4 and 5 respectively).\n",
    "\n",
    "<img src=\"\">\n",
    "<p style=\"text-align: center; font-weight:bold\"> Fig.4. Accuracy graphs of BLSTM network on MFCC features (w/o dropout)</p>\n",
    "\n",
    "<img src=\"\">\n",
    "<p style=\"text-align: center; font-weight:bold\"> Fig.5. Accuracy graphs of BLSTM network on MFCC features (w/dropout) </p>\n",
    "\n",
    "Apparently, dropout increases test set (unseen by the model) accuracy so we will add it to all consequent models regardless of their architecture.\n",
    "\n",
    "Next, we add convolutional layers in the beginning of the network and test on the same features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n",
      "\n",
      "Shape of the dataset: (43053, 1, 3277, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_11 (TimeDis (None, 1, 3277, 64)       1088      \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 1, 1638, 64)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 1, 1638, 64)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 1, 1638, 256)      524544    \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 432)               817344    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 1299      \n",
      "=================================================================\n",
      "Total params: 1,344,275\n",
      "Trainable params: 1,344,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "26752/43053 [=================>............] - ETA: 19:55 - loss: 1.0586 - acc: 0.4763"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-230127625e16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                  verbose=1)\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Conv1D, Dense\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import MaxPooling1D, Dropout, GlobalAveragePooling1D\n",
    "\n",
    "\n",
    "data_raw_1 = data_raw[:,::10]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_raw_1, labels, test_size=0.2, random_state=42)\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, int(X_train.shape[2])))\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, int(X_test.shape[2])))\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "print(\"\\nShape of the dataset:\", X_train.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=16, strides=1, padding=\"same\", activation='relu'),\n",
    "                          input_shape=X_train.shape[1:]))\n",
    "model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "model.add(TimeDistributed(Dropout(0.4)))\n",
    "# model.add(TimeDistributed(Conv1D(64, 64, padding=\"same\", activation='relu')))\n",
    "# model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "# model.add(TimeDistributed(Dropout(0.4)))\n",
    "# model.add(TimeDistributed(Conv1D(128, 64, padding=\"same\", activation='relu')))\n",
    "# model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "# model.add(TimeDistributed(Dropout(0.4)))\n",
    "# model.add(TimeDistributed(Conv1D(128, 64, padding=\"same\", activation='relu')))\n",
    "# model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "# model.add(TimeDistributed(Dropout(0.4)))\n",
    "# model.add(TimeDistributed(Conv1D(256, 64, padding=\"same\", activation='relu')))\n",
    "# model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "# model.add(TimeDistributed(Dropout(0.4)))\n",
    "model.add(TimeDistributed(Conv1D(256, 32, padding=\"same\", activation='relu')))\n",
    "model.add(TimeDistributed(GlobalAveragePooling1D()))\n",
    "model.add(Bidirectional(LSTM(216)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "test_callback = TestCallback((X_test, y_test), \"MFCC-CNN-BLSTM-drop\")\n",
    "\n",
    "hist = model.fit(X_train, y_train,\n",
    "                 callbacks=[test_callback],\n",
    "                 # validation_data=(X_1, y_1),\n",
    "                 epochs=100,\n",
    "                 batch_size=128,\n",
    "                 verbose=1)\n",
    "\n",
    "\n",
    "print(\"\\nEvaluating...\")\n",
    "y_pred = to_categorical(model.predict_classes(X_test, verbose=1))\n",
    "print(y_pred, y_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy:\", acc)\n",
    "rec = recall_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"Recall:\", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"\">\n",
    "<p style=\"text-align: center; font-weight:bold\"> Fig.4. Accuracy graphs of CNN-BLSTM network on raw audio (w/dropout)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x. References ##\n",
    "\n",
    "1. Gemmeke, Jort F., et al. \"Audio Set: An ontology and human-labeled dataset for audio events.\" *IEEE ICASSP*. 2017.\n",
    "2. AbdiÄ‡, Irman, et al. \"Detecting road surface wetness from audio: A deep learning approach.\" *Pattern Recognition (ICPR), 2016 23rd International Conference on. IEEE*, 2016.\n",
    "3. Prasad, Bhanu, and SR Mahadeva Prasanna, eds. Speech, audio, image and biomedical signal processing using neural networks. Vol. 83. Springer, 2007.\n",
    "4. Xiong, Ziyou, et al. \"Comparing MFCC and MPEG-7 audio features for feature extraction, maximum likelihood HMM and entropic prior HMM for sports audio classification.\" Multimedia and Expo, 2003. ICME'03. Proceedings. 2003 International Conference on. Vol. 3. IEEE, 2003.\n",
    "5. Scheirer, Eric, and Malcolm Slaney. \"Construction and evaluation of a robust multifeature speech/music discriminator.\" Acoustics, Speech, and Signal Processing, 1997. ICASSP-97., 1997 IEEE International Conference on. Vol. 2. IEEE, 1997.\n",
    "6. Lu, Lie, Stan Z. Li, and Hong-Jiang Zhang. \"Content-based audio segmentation using support vector machines.\" Proceedings of the IEEE International Conference on Multimedia and Expo (ICME 2001). 2001.\n",
    "7. Zhang, Tong, and C-C. Jay Kuo. \"Content-based classification and retrieval of audio.\" Advanced Signal Processing Algorithms, Architectures, and Implementations VIII. Vol. 3461. International Society for Optics and Photonics, 1998.\n",
    "8. Davis, Steven, and Paul Mermelstein. \"Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences.\" IEEE transactions on acoustics, speech, and signal processing 28.4 (1980): 357-366.\n",
    "9. Chu, Selina, Shrikanth Narayanan, and C-C. Jay Kuo. \"Environmental sound recognition with timeâ€“frequency audio features.\" IEEE Transactions on Audio, Speech, and Language Processing 17.6 (2009): 1142-1158.\n",
    "10. Casey, Michael. \"MPEG-7 sound-recognition tools.\" IEEE Transactions on circuits and Systems for video Technology 11.6 (2001): 737-747.\n",
    "11. Mathieu, Benoit, et al. \"YAAFE, an Easy to Use and Efficient Audio Feature Extraction Software.\" ISMIR. 2010.\n",
    "12. Eyben, Florian, Martin WÃ¶llmer, and BjÃ¶rn Schuller. \"Opensmile: the munich versatile and fast open-source audio feature extractor.\" Proceedings of the 18th ACM international conference on Multimedia. ACM, 2010.\n",
    "13. Eronen, Antti. \"Automatic musical instrument recognition.\" MÃ©moire de DEA, Tempere University of Technology (2001): 178.\n",
    "14. Duxbury, Chris, et al. \"Complex domain onset detection for musical signals.\" Proc. Digital Audio Effects Workshop (DAFx). Vol. 1. London: Queen Mary University, 2003.\n",
    "15. Makhoul, John. \"Linear prediction: A tutorial review.\" Proceedings of the IEEE 63.4 (1975): 561-580.\n",
    "16. BÃ¤ckstrÃ¶m, Tom, and Carlo Magi. \"Properties of line spectrum pair polynomialsâ€”A review.\" Signal processing 86.11 (2006): 3286-3298.\n",
    "17. Moore, Brian CJ, Brian R. Glasberg, and Thomas Baer. \"A model for the prediction of thresholds, loudness, and partial loudness.\" Journal of the Audio Engineering Society 45.4 (1997): 224-240.\n",
    "18. Peeters, Geoffroy. \"A large set of audio features for sound description (similarity and classification) in the CUIDADO project.\" (2004).\n",
    "19. Hochreiter, Sepp, and JÃ¼rgen Schmidhuber. \"Long short-term memory.\" Neural computation 9.8 (1997): 1735-1780.\n",
    "20. Eyben, Florian, et al. \"Universal onset detection with bidirectional long-short term memory neural networks.\" Proc. 11th Intern. Soc. for Music Information Retrieval Conference, ISMIR, Utrecht, The Netherlands. 2010.\n",
    "21. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
